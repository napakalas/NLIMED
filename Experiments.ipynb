{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiments setups\n",
    "\n",
    "If you not yet install NLIMED, please follow the [installation step](#scrollTo=rQWn0QqFZzM7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating required methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "    \n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from NLIMED import __file__\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "\n",
    "dest = os.path.join(os.path.dirname(__file__), 'indexes')\n",
    "source = source = 'pmr_inv'\n",
    "\n",
    "# type of parsers\n",
    "parsers = ['ncbo', 'benepar', 'stanza', 'coreNLP', 'xStanza']\n",
    "\n",
    "# indexing strategies\n",
    "idxMethods = ['wpure', 'wpl', 'wple']\n",
    "\n",
    "# term frequency calculation\n",
    "tfModes = ['mode1', 'mode2', 'mode3']\n",
    "\n",
    "def moveIndex(idxMode):\n",
    "    files = os.listdir(os.path.join(source,idxMode))\n",
    "    for file in files:\n",
    "        fileSrc = os.path.join(source,idxMode,file)\n",
    "        fileDest = os.path.join(dest,file)\n",
    "        shutil.copy(fileSrc,fileDest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: NLQ Annotator Performance\n",
    "\n",
    "This experiment is aimed to measure NLIMED performance in annotating natural language query into ontology classes. \n",
    "\n",
    "We have 52 data test consisting of queries and their related ontology classes annotated manually by expert. The queries have one to fourteen terms and one to four ontology class. This data test along with the experiment using jupyter is available at [Github](https://github.com/napakalas/NLIMED/tree/experiment).\n",
    "\n",
    "For the measurement, we calculate Area Under Curves of Precision and Recal ($AUC_{PR}$) for different parsers, term frequency calculations, and indexing strategies.\n",
    "\n",
    "Parsers:\n",
    "  * Benepar\n",
    "  * CoreNLP\n",
    "  * Stanza\n",
    "  * xStanza\n",
    "  * NCBO => cannot calculate $AUC_{PR}$\n",
    "\n",
    "Term frequency calculations:\n",
    "  * mode1: using similarity equation with all features and dependency level feature\n",
    "  * mode2: the tf is not utilising dependency\n",
    "  * mode3: using mode1 but the tf weight is selected for one feature with the highest value\n",
    "\n",
    "Indexing strategies:\n",
    "  * wpure: indexing all terms into each features\n",
    "  * wpl  : adding terms in prefered label to other features\n",
    "  * wple : adding terms in prefered label to empty features\n",
    "\n",
    "Please run **experiment setup** before proceeding this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance based on $AUC_{PR}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get $AUC_{PR}$ for all settings \n",
    "\n",
    "Beware, generating $AUC_{PR}$ **will take days**. \n",
    "\n",
    "If you just interested with the final result and the generated graph, you can jump to **the next subsection**.\n",
    "\n",
    "Since the generated data is quite big, we store it in files rather than manage it directly in the runtime data structure. Later we load the files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating AUC_PR\n",
    "from itertools import product\n",
    "for idxMethod, tfMode, parser in product(idxMethods, tfModes, parsers):\n",
    "    if parser == 'ncbo': continue\n",
    "    mode = int(tfMode[-1])\n",
    "    moveIndex(idxMethod)\n",
    "    nli = NLIMED(repo='pmr', parser=parser, tfMode=mode)\n",
    "    stats = nli.auc('dataSource/DataTest.json')\n",
    "    # save to Json\n",
    "    filename = idxMethod + '_auc_' + tfMode + '_' + parser\n",
    "    filename = \"saveFile/\"+filename + \"_\" + str(time.time()) + \".json\"\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(stats, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all $AUC_{PR}$ from files\n",
    "\n",
    "Loading large amount of files to dictionary will take minutes. The following code will load $AUC_{PR}$ files and save their summary into a summary and dataframe files. If the summary and dataframe files are availabel, the code will load it rather than the $AUC_{PR}$ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time, gc\n",
    "\n",
    "def loadAucPRData(filename = \"\"):\n",
    "    columns = ['idxMethod', 'tfMode', 'parser', '\\u03B1 (preferred label)', '\\u03B2 (synonym)', '\\u03B3 (definition)', '\\u03B4 (parent label)', '\\u03B8 (description)', 'auc']\n",
    "    dfAucPR = pd.DataFrame(columns=columns)\n",
    "    dataAucPR = {}\n",
    "    \n",
    "    if len(filename)>0: ## load from a summary file\n",
    "        try:\n",
    "            with open('saveFile/'+filename+'.json', 'r') as fp:\n",
    "                dataAucPR = json.load(fp)\n",
    "            dfAucPR = pd.read_csv('saveFile/'+filename+'.csv')\n",
    "            return dataAucPR, dfAucPR\n",
    "        except:\n",
    "            print(\"\\x1b[31m\\\"File \"+filename+\" is not exist\\\"\\x1b[0m\")\n",
    "    \n",
    "    with open(os.path.join('saveFile','pure_10_ncbo_1613804506.2163131.json'), 'r') as fp:\n",
    "        dataNcbo = json.load(fp)\n",
    "    files = sorted([file for file in os.listdir('saveFile') if '_auc_mode' in file], reverse=False)\n",
    "    files = list({file.rsplit('_', 1)[0]:file for file in files}.values())\n",
    "    for file in files:\n",
    "        idxMethod, tfMode, parser = [file.split('_')[0]]+file.split('_')[2:4]\n",
    "        data = dataAucPR[idxMethod] if idxMethod in dataAucPR else {}\n",
    "        if 'ncbo' not in data: data['ncbo'] = dataNcbo\n",
    "        if tfMode not in data: data[tfMode] = {}\n",
    "        if parser not in data[tfMode]:\n",
    "            start = time.time()\n",
    "            print(\"Loading .. %s, %s, %s, %s\"%(idxMethod, parser, tfMode, file))\n",
    "            with open(os.path.join('saveFile',file), 'r') as fp:\n",
    "                loaded = json.load(fp)\n",
    "            idx = loaded['settings'].index(loaded['maxAuc100']['settings'][0])\n",
    "            data[tfMode][parser] = {'maxAuc100':loaded['maxAuc100'], 'maxAuc70':loaded['maxAuc70'], 'maxAuc50':loaded['maxAuc50'], 'cutoffs':loaded['cutoffs'][idx]}\n",
    "            print(\"Finish loading in %d seconds, start store to df\"%(time.time()-start))\n",
    "            \n",
    "            #load to dataframe\n",
    "            pref = [[idxMethod, tfMode, parser]] * len(loaded['settings'])\n",
    "            suff = [loaded['auc100']]\n",
    "            from operator import itemgetter\n",
    "            itg = itemgetter(*filter((len(loaded['settings'][0])-1).__ne__, range(len(loaded['settings'][0]))))\n",
    "            med = list(map(list, map(itg, loaded['settings'])))\n",
    "            m = map(list.__add__, pref, med)\n",
    "            l = list(map(list, zip(*m)))+suff\n",
    "            d = list(map(list,zip(*l)))\n",
    "            dfAucPR = dfAucPR.append(pd.DataFrame(data=d, columns=columns))\n",
    "            \n",
    "            del loaded\n",
    "            print(\"Finished in %d seconds\"%(time.time()-start))\n",
    "        dataAucPR[idxMethod] = data\n",
    "    \n",
    "    # save dataAucPR to file\n",
    "    with open('saveFile/'+filename+'.json', 'w') as fp:\n",
    "        json.dump(dataAucPR, fp)\n",
    "    # save dfAucPR to file\n",
    "    dfAucPR.to_csv('saveFile/'+filename+'.csv')\n",
    "    print('Done loading data ...')\n",
    "    return dataAucPR, dfAucPR\n",
    "\n",
    "dataAucPR, dfFeatAnalysis = loadAucPRData('nlimedSummaryResults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the interpolated plot for precision and recall ($AUC_{PR}$ graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "\n",
    "def drawAllAucPR(isVerbose=False, isInterpolated=False, isLatex=False):\n",
    "    tfModes = {'mode1':'mode_1', 'mode2':'mode_2', 'mode3':'mode_3'}\n",
    "    idxMethods = {'wpure', 'wpl', 'wple'}\n",
    "    aucTypes = ['maxAuc100']\n",
    "    fontsize = 14\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 3,figsize=(15,15), sharex=True, sharey=True)\n",
    "        \n",
    "    linestyles = [(0, (3, 5, 1, 5)), (0, (3, 1, 1, 1)), (0, (1, 1)), (0, (5, 1)), (0, (5, 5)), (0, (3, 1))]\n",
    "    for aucType in aucTypes:\n",
    "        print(aucType)\n",
    "        prevMode, prevMethod = '',''\n",
    "        for row, idxMethod in enumerate(idxMethods):\n",
    "            for col, tfMode in enumerate(tfModes):\n",
    "                if tfMode not in dataAucPR[idxMethod]: continue\n",
    "                for count, parser in enumerate(parsers):\n",
    "                    if parser in dataAucPR[idxMethod][tfMode]:\n",
    "                        settings = dataAucPR[idxMethod][tfMode][parser][aucType]['settings']\n",
    "                        maxAlpha = max(list(map(list, zip(*settings)))[0])\n",
    "                        recalls = dataAucPR[idxMethod][tfMode][parser][aucType]['recalls']\n",
    "                        precisions = dataAucPR[idxMethod][tfMode][parser][aucType]['precisions']\n",
    "                        if isInterpolated:\n",
    "                            i=len(recalls)-2\n",
    "                        # interpolation...\n",
    "                        while i>=0:\n",
    "                            if precisions[i+1]>precisions[i]:\n",
    "                                precisions[i]=precisions[i+1]\n",
    "                            i=i-1\n",
    "                        \n",
    "                        axs[row,col].plot(recalls[:],precisions[:],linestyle=linestyles[count],label=parser,linewidth=1.5)\n",
    "                        \n",
    "                        if isVerbose:\n",
    "                            print('%s:%f, \\tsetting rate:'%(parser,dataAucPR[idxMethod][tfMode][parser][aucType]['auc']), end=' ')\n",
    "                            print(*map(mean, zip(*settings)))\n",
    "                            print('\\trecalls',recalls[-20:])\n",
    "                            print('\\tprecisions',precisions[-20:])\n",
    "                        header=''\n",
    "                        header = idxMethod + ' & ' if idxMethod != prevMethod else ' & '\n",
    "                        header += tfModes[tfMode].replace('_','\\_') + ' & ' if tfMode != prevMode else ' & '\n",
    "                        prevMode, prevMethod = tfMode, idxMethod\n",
    "                        if isLatex:\n",
    "                            print(header, parser, ' & ', tuple(map(mean, zip(*settings)))[:-1], ' & ', round(dataAucPR[idxMethod][tfMode][parser][aucType]['auc'],3), ' \\\\\\\\')\n",
    "        \n",
    "                ncboPrecision = dataAucPR[idxMethod]['ncbo']['maxSetting']['precision']\n",
    "                ncboRecall = dataAucPR[idxMethod]['ncbo']['maxSetting']['recall']\n",
    "                axs[row,col].plot(ncboRecall,ncboPrecision,label='ncbo',marker=\".\", markersize=15)\n",
    "                axs[row,col].grid()\n",
    "                if row==0:\n",
    "                    axs[0,col].text(0.5, 1.075, tfModes[tfMode], ha='center', fontweight='bold',size=fontsize)\n",
    "                if col==2:\n",
    "                    axs[row,2].text(1, 0.57, idxMethod, va='center', rotation=270, fontweight='bold',size=fontsize)\n",
    "                \n",
    "    print('ncbo precision: %f, recall: %f'%(ncboRecall,ncboPrecision))\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xlabel('recall', fontsize=fontsize)\n",
    "        ax.set_ylabel('precision', fontsize=fontsize)\n",
    "\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.88, top=0.88) \n",
    "    fig.legend(handles, labels, loc=7, fontsize='large')\n",
    "    fig.savefig('saveFigures/PR-AUC.pdf',dpi=10000)            \n",
    "    \n",
    "drawAllAucPR(isVerbose=False, isInterpolated=True, isLatex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The role of feature\n",
    "\n",
    "We are sure that preferred label and synonym are the most significant features, therefore, now we investigate the role of other features, i.e. definition, parent label, and model description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method to draw the role features \n",
    "def drawPlotPairGrid(idxMethod, tfMode, parser):\n",
    "    print(parser)\n",
    "    variables = ['\\u03B2 (synonym)', '\\u03B3 (definition)', '\\u03B4 (parent label)', '\\u03B8 (description)']\n",
    "    df = dfFeatAnalysis\n",
    "    df = df.loc[(df['parser'] == parser)&(df['idxMethod'] == idxMethod)&(df['tfMode'] == tfMode)]\n",
    "    \n",
    "    g = sns.PairGrid(df, hue=\"auc\", vars=variables, corner=True)\n",
    "    g.map_diag(sns.kdeplot, lw=3, hue=None, color=\"0.4\")\n",
    "    g.map_offdiag(sns.scatterplot)\n",
    "    g.add_legend()\n",
    "    plt.savefig(\"saveFigures/feature_role_\" + idxMethod + '_' + parser + '.pdf',dpi=300)\n",
    "\n",
    "# draw the role for all setings\n",
    "from itertools import product\n",
    "for idxMethod, tfMode, parser in product(idxMethods, tfModes, parsers):\n",
    "    if parser == 'ncbo': continue\n",
    "    drawPlotPairGrid(idxMethod,tfMode,parser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the role of feature (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPlot3DHeatmap(idxMethod, parser):\n",
    "    df = dfFeatAnalysis\n",
    "    df = df.loc[(df['parser'] == parser)&(df['idxMethod'] == idxMethod)&(df['tfMode'] == 'mode3')]\n",
    "    # Creating dataset\n",
    "    g = df['\\u03B3 (definition)'].tolist()\n",
    "    d = df['\\u03B4 (parent label)'].tolist()\n",
    "    t = df['\\u03B8 (description)'].tolist()\n",
    "    auc = df['auc'].tolist()\n",
    "    \n",
    "\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize = (16, 9))\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "    # Add x, y gridlines\n",
    "    ax.grid(b = True, color ='grey',\n",
    "            linestyle ='-.', linewidth = 0.3,\n",
    "            alpha = 0.2)\n",
    "\n",
    "\n",
    "    # Creating color map\n",
    "    my_cmap = plt.get_cmap('twilight')\n",
    "\n",
    "    # Creating plot\n",
    "    sctt = ax.scatter3D(g, d, t,\n",
    "                        alpha = 0.8,\n",
    "                        c = auc,\n",
    "                        cmap = my_cmap,\n",
    "                        marker ='o')\n",
    "\n",
    "    plt.title(parser)\n",
    "    ax.set_xlabel('\\u03B3 (definition)', fontweight ='bold')\n",
    "    ax.set_ylabel('\\u03B4 (parent label)', fontweight ='bold')\n",
    "    ax.set_zlabel('\\u03B8 (description)', fontweight ='bold')\n",
    "    fig.colorbar(sctt, ax = ax, shrink = 0.5, aspect = 5, )\n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"saveFigures/three_feature_role_\" + idxMethod + '_' + parser + '.pdf',dpi=400)\n",
    "    \n",
    "drawPlot3DHeatmap('wpl','coreNLP')\n",
    "drawPlot3DHeatmap('wpl','benepar')\n",
    "drawPlot3DHeatmap('wpl','stanza')\n",
    "drawPlot3DHeatmap('wpl','xStanza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance based on query length\n",
    "\n",
    "To annotated phrases in query to an ontology class, we calculate the degree of association where higher value is more likely to be the relevant annotation. However, there is no guidance for the exact degree of association. Therefore we investigate the best minimum value or cutoff of the degree of association. Using the best cuoff, we measure NLQ Annotator based on the query length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Get the best cutoff for WPL and Mode3\n",
    "\n",
    "We focus on WPL and Mode3 because they show the highest performance. \n",
    "\n",
    "If you just interested with the final result and the generated graph, you can jump to **the next subsection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompleteSettingAndCutoff(idxType, tfMode, parser, recallType, limit=0):\n",
    "    print(idxType, tfMode, parser, recallType)\n",
    "    maxResult = dataAucPR[idxType][tfMode][parser][recallType]\n",
    "    meanMaxSetting = np.mean(np.array(maxResult['settings']),axis=0)\n",
    "    idx = dataAucPR[idxType][tfMode][parser]['maxAuc100']['settings'].index(maxResult['settings'][0])\n",
    "    precisions = dataAucPR[idxType][tfMode][parser][recallType]['precisions']\n",
    "    recalls = dataAucPR[idxType][tfMode][parser][recallType]['recalls']\n",
    "    cutoffs = dataAucPR[idxType][tfMode][parser]['cutoffs'][:len(recalls)]\n",
    "    data = []\n",
    "    for i, cutoff in enumerate(cutoffs):\n",
    "        fmeasure = (precisions[i]*recalls[i])/(precisions[i]+recalls[i])*2\n",
    "        precLimit = 1/(1+(recalls[i]*limit))\n",
    "        data += [{'cutoff':cutoff, 'recall':recalls[i], 'precision':precisions[i], 'fmeasure':fmeasure, 'preclimit':precLimit}]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(['fmeasure', 'precision'], ascending=False)\n",
    "    \n",
    "    # get rate of cutoff and return number\n",
    "    maxFmeasure = df['fmeasure'][:50].max()\n",
    "    dfMax = df.loc[df['fmeasure'] == maxFmeasure]\n",
    "    maxCutoff = dfMax['cutoff'].map(lambda x: x[0]).mean()\n",
    "    maxReturn = math.floor(dfMax['cutoff'].map(lambda x: x[1]).mean())\n",
    "    \n",
    "    setting = list(meanMaxSetting[:-1]) + [maxCutoff, maxReturn]\n",
    "    return df, setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising to get result best setting\n",
    "with open('dataSource/DataTest.json','r') as fp:\n",
    "    dataTest = json.load(fp)\n",
    "\n",
    "def _getURICode(uri):\n",
    "    # function to get ontology class ID rather than full url\n",
    "    import re\n",
    "    partUri = uri[uri.rfind('/')+1:].lower()\n",
    "    regex = re.compile('[^a-z0-9]')\n",
    "    partUri = regex.sub('', partUri)\n",
    "    return partUri\n",
    "\n",
    "resultBestSetting = {'benepar':{}, 'stanza':{}, 'coreNLP':{}, 'xStanza':{}, 'ncbo':{}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat this command several time until you get all data. \n",
    "# Sometime ncbo services is not availabel due to inactive server.\n",
    "# It will take approximately 10 minutes to run\n",
    "for parser in parsers:\n",
    "    nli = NLIMED(repo='pmr', parser=parser, quite=True)\n",
    "    if 'class' in resultBestSetting[parser]:\n",
    "        if len(resultBestSetting[parser]['class']) > 0: continue\n",
    "    if parser != 'ncbo':\n",
    "        df, setting = getCompleteSettingAndCutoff('wpl','mode3',parser,'maxAuc100',1)\n",
    "        setting += [3] #mode_3 for selecting the highest feature only\n",
    "        nli.setWeighting(*setting)\n",
    "        print('Best setting: ', setting)\n",
    "    handler = resultBestSetting[parser]\n",
    "    handler.update({'term':{},'class':{}})\n",
    "\n",
    "    # sometime, ncbo services is not available\n",
    "    try:\n",
    "        numCorrect, numReturn, numPositive = 0,0,0\n",
    "        for count, queryTest in enumerate(list(dataTest.values())):\n",
    "            \n",
    "            annotation = nli.getAnnotated(queryTest['query'])\n",
    "            queryTest['annotation'] = [_getURICode(cls) for cls in queryTest['annotation']]\n",
    "            numOfClasses = len(queryTest['annotation'])\n",
    "            numOfTerms = len(queryTest['query'].split())\n",
    "            numPositive += numOfClasses\n",
    "            \n",
    "            if numOfClasses not in handler['class']: handler['class'][numOfClasses]=[]\n",
    "                \n",
    "            if numOfTerms not in handler['term']: handler['term'][numOfTerms]=[]\n",
    "            \n",
    "            if len(annotation['result']) > 0:\n",
    "                annon = [_getURICode(d) for e in annotation['result'] for d in e[0]]\n",
    "                numAnnonCorrect = len(set(queryTest['annotation']) & set(annon))\n",
    "                numCorrect += numAnnonCorrect\n",
    "                numReturn += len(set(annon))\n",
    "                handler['class'][numOfClasses] += [(numAnnonCorrect,len(set(annon)))]\n",
    "                handler['term'][numOfTerms] += [(numAnnonCorrect,len(set(annon)))]\n",
    "            else:\n",
    "                handler['class'][numOfClasses] += [(0,0)]\n",
    "                handler['term'][numOfTerms] += [(0,0)]\n",
    "                \n",
    "            if count%10==0: print(count,end=' ') \n",
    "        precision = numCorrect/numReturn\n",
    "        recall = numCorrect/numPositive\n",
    "\n",
    "        print('\\nrecall ',recall, numCorrect, numPositive)\n",
    "        print('precision',precision, numCorrect, numReturn)\n",
    "        print('f-measure', precision*recall*2/(precision+recall))\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# save result\n",
    "with open('saveFile/resultBestSetting.json', 'w') as fp:\n",
    "    json.dump(resultBestSetting, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Load the analysis result\n",
    "\n",
    "In a case you just need the result analysis, load this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if you already saved the best setting, you can just load it\n",
    "with open('saveFile/resultBestSetting.json', 'r') as fp:\n",
    "    resultBestSetting = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the performance plot based on query length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPlot(stats,statType,xlabel):\n",
    "    nums, precs, recs, fmeas, group = [], [], [], [], []\n",
    "    \n",
    "    df = pd.DataFrame(columns=['parser','statType', 'precision', 'recall', 'fmeasure', 'number'])\n",
    "    \n",
    "    for label, stat in stats.items():\n",
    "        st = {int(k):v for k,v in stat[statType].items()}\n",
    "        st = dict(sorted(st.items()))\n",
    "        for k, v in st.items():\n",
    "            t_v = list(map(list, zip(*v)))\n",
    "            prec = sum(t_v[0])/sum(t_v[1]) if sum(t_v[1])>0 else 1.\n",
    "            rec = sum(t_v[0])/(int(k)*len(v))\n",
    "            try:\n",
    "                fmeasure = prec * rec *2 /(prec+rec)\n",
    "            except:\n",
    "                fmeasure = 0\n",
    "            df.loc[len(df.index)] = [label, 'ontology class' if statType=='class' else statType, prec, rec, fmeasure, str(k)]\n",
    "\n",
    "    sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "    \n",
    "    p = sns.relplot(x='number',y='fmeasure', \n",
    "                hue='parser', aspect=1, height=4, \n",
    "                size='precision',\n",
    "                col='statType',\n",
    "                data=df,\n",
    "                sizes=(10, 200),\n",
    "               )\n",
    "    p.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "    p._legend.set_title('')\n",
    "    plt.savefig('saveFigures/'+xlabel + '.pdf',dpi=300)\n",
    "#     plt.grid()\n",
    "    \n",
    "\n",
    "drawPlot(resultBestSetting, 'class', 'number of phrases per query')\n",
    "drawPlot(resultBestSetting, 'term', 'number of terms per query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 2. Experiment 2: NLIMED Behaviour on Native Query in PMR (Historical Data)\n",
    "\n",
    "We have collect historical data from the PMR query logs regarding user query in its search feature and the biological models relevant to the query.\n",
    "\n",
    "The data is differentiated based on the co-occurence of terms in the query and the ontology classes in the relevant model. There are three types of query-model pairs:\n",
    "  * 0 co-occurrence\n",
    "  * 0 < co-occurrence <= 0.5\n",
    "  * 0.5 < co-occurrence\n",
    "\n",
    "Please run **experiment setup** before proceeding this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows PMR statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology dictionaries\n",
    "from nltk.corpus import stopwords\n",
    "import gzip, pickle\n",
    "file = gzip.GzipFile('dataSource/pmr_onto.gz', 'rb')\n",
    "ontologies = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Organise K,V where K is ClassID, V is a set of terms in the ontology class\n",
    "ontoClasses = {}\n",
    "for ontoName, ontology in ontologies.items():\n",
    "    for classID, features in ontology['data'].items():\n",
    "        classID = (''.join(re.split('_|:', classID))).replace('OPB#','')\n",
    "        feats = [features[0]] + features[1]\n",
    "        terms = []\n",
    "        for feat in feats:\n",
    "            if isinstance(feat, str):\n",
    "                terms += [term for term in feat.lower().split()]\n",
    "        terms = set(terms) - stopwords\n",
    "        ontoClasses[classID] = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised link related to model\n",
    "def normalisedLink(link):\n",
    "    spts = link.split('/rawfile/')\n",
    "    if len(spts) > 1:\n",
    "        return '/'.join([spts[0],'rawfile','HEAD',spts[-1][spts[-1].find('/')+1:]])\n",
    "    return link\n",
    "\n",
    "# Normalised full format ClassID to shorter ClassID\n",
    "def getShortID(bioClass):\n",
    "    txtClass = bioClass.replace('<', '').replace('>', '').strip(' \\t\\n\\r')\n",
    "    if txtClass[0:4] == 'http':\n",
    "        oboId = txtClass[txtClass.rfind('/') + 1:]\n",
    "        if any(x in oboId for x in ['_', ':']):\n",
    "            oboId = oboId.replace('_', ':')\n",
    "            return oboId[0:oboId.find(':')] + oboId[oboId.find(':') + 1:]\n",
    "    return ''\n",
    "\n",
    "# Show PMR statistic\n",
    "print(\"\"\"STATISTIC PMR\"\"\")\n",
    "with open('dataSource/listOfObjects.txt', 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "print(\"Number of objects: \",len(lines))\n",
    "newLines = set([line[:-1] for line in lines if line[:4]=='http' and 'models' not in line])\n",
    "print(\"Number of ontology links: \",len(newLines))\n",
    "newLines = set([line[line.rfind('/')+1:].replace(':','_') for line in newLines])\n",
    "print(\"Number of distinct ontologies: \",len(newLines),\"\\n\")\n",
    "with open('dataSource/rdfPaths.json', 'r') as fp:\n",
    "    rdfPaths = json.load(fp)\n",
    "print(\"Number of links with rdf\", len(rdfPaths))\n",
    "\n",
    "# Organise link annotated with ontology classes to validLinks\n",
    "# Organise ontology classes used in link to validOnto\n",
    "validLinks = {}\n",
    "validOnto = []\n",
    "for rdfPath in rdfPaths:\n",
    "    triples = rdfPath['paths']\n",
    "    for triple in triples:\n",
    "        if triple['o'][:4] == 'http' and 'models.phys' not in triple['o']:\n",
    "            # normalised the link\n",
    "            link = normalisedLink(rdfPath['link'])\n",
    "            classID = getShortID(triple['o'])\n",
    "            if classID in ontoClasses:\n",
    "                if link not in validLinks: validLinks[link] = {'terms':[],'classes':{}}\n",
    "                validLinks[link]['terms'] += ontoClasses[classID]\n",
    "                validLinks[link]['classes'][classID] = ontoClasses[classID]\n",
    "                validOnto += [classID]\n",
    "validOnto = set(validOnto)\n",
    "for k in validLinks.keys(): \n",
    "    validLinks[k]['terms'] = set(validLinks[k]['terms'])\n",
    "print(\"Number of models annotated to ontology class: \",len(validLinks))\n",
    "print(\"Number of URIs Ontology Classes: \",len(validOnto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Data test preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query and answers from query log\n",
    "site = 'https://models.physiomeproject.org/'\n",
    "with open('dataSource/query_workspace.json','r') as fp:\n",
    "    queryWorkspaces = json.load(fp)\n",
    "print('The total number of raw query and answer is: ',len(queryWorkspaces))\n",
    "\n",
    "# Normalise all query and answers\n",
    "for k, v in queryWorkspaces.items():\n",
    "    for i in range(len(v)-1,-1,-1):\n",
    "        v[i] = normalisedLink(v[i])\n",
    "\n",
    "# Load cluster\n",
    "with open('dataSource/cellmlClusterer.json','r') as fp:\n",
    "    cellmlClusters = json.load(fp)\n",
    "    \n",
    "# Enrich query and answers with cluster\n",
    "for k, v in queryWorkspaces.items():\n",
    "    links = set(v)\n",
    "    for link in v:\n",
    "        if link in cellmlClusters['url2Cluster']:\n",
    "            links.update(set(cellmlClusters['cluster'][cellmlClusters['url2Cluster'][link]]))\n",
    "    queryWorkspaces[k] = links\n",
    "\n",
    "# Remove query and answers where the answer is not annotated with ontology classes\n",
    "for k, v in queryWorkspaces.copy().items():\n",
    "    for link in v.copy():\n",
    "        if (site + link) not in validLinks: \n",
    "            v.remove(link)\n",
    "    if len(v) == 0:\n",
    "        queryWorkspaces.pop(k)\n",
    "\n",
    "print('The number of query and answer annotated with ontology classes: ',len(queryWorkspaces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for queries having terms indexed by ontology classes (Considering preferred label and synonym only)\n",
    "nli = NLIMED(repo='pmr', parser='CoreNLP', pl=1, alpha=3, beta=2, gamma=0, delta=0, theta=0, cutoff=0, tfMode=1, quite=True)\n",
    "\n",
    "native_QR_PMR = {}\n",
    "for count, (k, v) in enumerate(queryWorkspaces.items()):\n",
    "    annotated = nli.getAnnotated(k)\n",
    "    if len(annotated['result']) > 0:\n",
    "        native_QR_PMR[k] = {link:validLinks[site+link] for link in v if site+link in validLinks}\n",
    "        \n",
    "    if count%20==0: print(count,end=' ')\n",
    "\n",
    "print('The number of query and answes annotated with ontology classes, considering preferred label and synonym only: ', len(native_QR_PMR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter native query to remove a model where it's class ontology features do not contain terms in its query\n",
    "onto_native_QR_PMR = {}\n",
    "for q, vals in native_QR_PMR.items():\n",
    "    for link in vals:\n",
    "        if site+link in validLinks:\n",
    "            for t in q.split():\n",
    "                if t in validLinks[site+link]['terms']:\n",
    "                    onto_native_QR_PMR[q] = native_QR_PMR[q]\n",
    "print('The number of query and answer where terms in ontology classes appear in query: ', len(onto_native_QR_PMR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a class with maximum proportion of terms in query to terms in class ontology\n",
    "# onto_native_QR_PMR_max ==> query to max poportion in onto_native_QR_PMR\n",
    "# native_QR_PMR_max ==> query to max poportion in native_QR_PMR\n",
    "\n",
    "onto_native_QR_PMR_max = {}\n",
    "for q, v in onto_native_QR_PMR.items():\n",
    "    onto_native_QR_PMR_max[q] = 0\n",
    "    qTerms = set(q.split())\n",
    "    for link in v:\n",
    "        data = validLinks[site+link]\n",
    "        for classID, classTerms in data['classes'].items():\n",
    "            if len(qTerms&classTerms)/len(classTerms) > 0.: \n",
    "                if len(qTerms&classTerms)/len(classTerms) > onto_native_QR_PMR_max[q]:\n",
    "                    onto_native_QR_PMR_max[q] = len(qTerms&classTerms)/len(classTerms)\n",
    "\n",
    "native_QR_PMR_max = onto_native_QR_PMR_max.copy()\n",
    "native_QR_PMR_max.update({q:0 for q in native_QR_PMR if q not in onto_native_QR_PMR_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save/load the founded query-results pairs from query logs\n",
    "if 'native_QR_PMR' in globals():\n",
    "    # modify set data type in pairs to list so it can be saved to json\n",
    "    native_QR_PMR = {k:list(v) for k, v in native_QR_PMR.items()}\n",
    "    with open('saveFile/native_QR_PMR.json', 'w') as fp:\n",
    "        json.dump(native_QR_PMR, fp)\n",
    "else:\n",
    "    with open('saveFile/native_QR_PMR.json', 'r') as fp:\n",
    "        native_QR_PMR = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get query results utilising CoreNLP NLIMED (supposed to be the best setting)\n",
    "def getNliResults(nli):\n",
    "    nliResults = {}\n",
    "    for i, (k, v) in enumerate(native_QR_PMR.items()):\n",
    "        if i%20==0: print(i, end=' ')\n",
    "        models = nli.getModels(k)\n",
    "        nliResults[k] = []\n",
    "        for result in models['results']:\n",
    "            model = result['graph'].replace('https://models.physiomeproject.org/','') \\\n",
    "                    + '/rawfile/HEAD/' + result['Model_entity'][:result['Model_entity'].find('#')]\n",
    "            if model not in nliResults[k] and not model.endswith('sedml'): nliResults[k] += [model]\n",
    "    return nliResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mean Average Precision (MAP)## GET MEAN AVERAGE PRECISION (MAP)\n",
    "def getMAP(results, reverence, k, isVerbose = False):\n",
    "    totIdentify = 0\n",
    "    totAP = 0\n",
    "    for query, relModels in reverence.items():\n",
    "        relevances = [0]*k\n",
    "        totRelevant = 0\n",
    "        ap = 0\n",
    "        for i in range(k):\n",
    "            if len(results[query]) > i:\n",
    "                if results[query][i] in relModels:\n",
    "                    relevances[i] = 1\n",
    "                    ap += sum(relevances[0:i+1])/(i+1)\n",
    "                    if isVerbose:\n",
    "                        print(i+1, ap, sum(relevances[0:i+1]),relevances, query)\n",
    "                    totRelevant += 1\n",
    "        if totRelevant > 0:\n",
    "            totAP += ap/totRelevant\n",
    "            totIdentify += 1\n",
    "    return {'map':totAP/len(reverence), 'rate':totIdentify/len(reverence), 'identify':totIdentify, 'totRel':len(reverence)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean Average Precision (MAP) for each setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLIMED best setting from nlimed summary results\n",
    "with open('saveFile/nlimedSummaryResults.json', 'r') as fp:\n",
    "    nliSettings = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis of the best cutoff and the number of return\n",
    "# Returning dataframe of cutoff and number return and the best setting\n",
    "dataAucPR = nliSettings\n",
    "def analyseData(idxType, tfMode, parser, recallType, limit=0):\n",
    "    print(idxType, tfMode, parser, recallType)\n",
    "    maxResult = dataAucPR[idxType][tfMode][parser][recallType]\n",
    "    print('Best settings: ',maxResult['settings'])\n",
    "    meanMaxSetting = np.mean(np.array(maxResult['settings']),axis=0)\n",
    "    print('Mean of best setting: ', meanMaxSetting)\n",
    "    idx = dataAucPR[idxType][tfMode][parser][recallType]['settings'].index(maxResult['settings'][0])\n",
    "    precisions = dataAucPR[idxType][tfMode][parser][recallType]['precisions']\n",
    "    recalls = dataAucPR[idxType][tfMode][parser][recallType]['recalls']\n",
    "    cutoffs = dataAucPR[idxType][tfMode][parser]['cutoffs'][:len(recalls)]\n",
    "    data = []\n",
    "    for i, cutoff in enumerate(cutoffs):\n",
    "        fmeasure = (precisions[i]*recalls[i])/(precisions[i]+recalls[i])*2\n",
    "        precLimit = 1/(1+(recalls[i]*limit))\n",
    "        data += [{'cutoff':cutoff, 'recall':recalls[i], 'precision':precisions[i], 'fmeasure':fmeasure, 'preclimit':precLimit}]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(['fmeasure', 'precision'], ascending=False)\n",
    "    \n",
    "    # get rate of cutoff and return number\n",
    "    maxFmeasure = df['fmeasure'][:50].max()\n",
    "    dfMax = df.loc[df['fmeasure'] == maxFmeasure]\n",
    "    maxCutoff = dfMax['cutoff'].map(lambda x: x[0]).mean()\n",
    "    maxReturn = math.floor(dfMax['cutoff'].map(lambda x: x[1]).mean())\n",
    "    maxCutoff, maxReturn\n",
    "    \n",
    "    setting = list(meanMaxSetting[:-1]) + [maxCutoff, maxReturn]\n",
    "    return df, setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results from NCBO\n",
    "nliNcbo = NLIMED(repo='pmr', parser='ncbo')\n",
    "nliNcboResult = getNliResults(nliNcbo)\n",
    "ncboMap5 = getMAP(nliNcboResult, native_QR_PMR,5)\n",
    "ncboMap10 = getMAP(nliNcboResult, native_QR_PMR,10)\n",
    "\n",
    "ncboMap1000 = getMAP(nliNcboResult, native_QR_PMR,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MAP for each setting\n",
    "\n",
    "dest = os.path.join(os.path.dirname(__file__), 'indexes')\n",
    "source = os.path.join('pmr_inv')\n",
    "\n",
    "def getMapForAllSettings(isBest = False, cutoff=0, pl=0, multipliers=None, resultSettings={}):\n",
    "    for idxMode, v1 in resultSettings.items():\n",
    "        if idxMode=='ncbo': continue\n",
    "        moveIndex(idxMode)\n",
    "        for tfMode, v2 in v1.items():\n",
    "            for parser, v3 in v2.items():\n",
    "                if isinstance(v3,list) or 'maxAuc100' not in v3: continue\n",
    "                df, setting = analyseData(idxMode, tfMode, parser,'maxAuc100')\n",
    "                if pl > 0: setting[-1] = pl\n",
    "                if not isBest:\n",
    "                    setting[-2] = cutoff\n",
    "                    if multipliers != None:\n",
    "                        for i, v in enumerate(multipliers): setting[i]=v\n",
    "                if 'results' in v3['maxAuc100']: \n",
    "                    print(idxMode, tfMode, parser, setting, 'SKIPPED')\n",
    "                    continue \n",
    "                print(idxMode, tfMode, parser, setting)\n",
    "                nli = NLIMED(repo='pmr', parser=parser)\n",
    "                nli.setWeighting(*setting)\n",
    "                nliResult = getNliResults(nli)\n",
    "                v3['maxAuc100']['results'] = nliResult\n",
    "                v3['maxAuc100']['map5'] = getMAP(nliResult, native_QR_PMR,5)\n",
    "                v3['maxAuc100']['map10'] = getMAP(nliResult, native_QR_PMR,10)\n",
    "                v3['maxAuc100']['map1000'] = getMAP(nliResult, native_QR_PMR,1000)\n",
    "                print('\\nMAP5:',v3['maxAuc100']['map5'],' \\tMAP10', v3['maxAuc100']['map10'],'\\n')\n",
    "    # Set result from NCBO\n",
    "    resultSettings['ncbo'] = {'results':nliNcboResult, 'map5':ncboMap5, 'map10':ncboMap10, 'map1000':ncboMap1000}\n",
    "    # Save to file\n",
    "    addName = '_'+'_'.join(str(m) for m in multipliers) if multipliers != None else ''\n",
    "    addName += '_best' if isBest else '_'+str(cutoff)\n",
    "    addName += '_best' if pl == 0 else '_'+str(pl)\n",
    "    with open('saveFile/nlimedSummaryResultsComplete'+addName+'.json', 'w') as fp:\n",
    "        json.dump(resultSettings, fp)\n",
    "        \n",
    "# initialised resultSettings\n",
    "resultSettings = []\n",
    "for i in range(6):\n",
    "    resultSettings += [copy.deepcopy(nliSettings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMapForAllSettings(isBest=True, resultSettings=resultSettings[0])\n",
    "getMapForAllSettings(isBest=True, pl=5, resultSettings=resultSettings[1])\n",
    "getMapForAllSettings(isBest=False, cutoff=0, pl=5, resultSettings=resultSettings[2])\n",
    "getMapForAllSettings(isBest=False, cutoff=0, pl=0, resultSettings=resultSettings[3])\n",
    "getMapForAllSettings(isBest=False, cutoff=0, pl=5, multipliers=[3,3,1,1,1], resultSettings=resultSettings[4])\n",
    "getMapForAllSettings(isBest=False, cutoff=0, pl=5, multipliers=[3,3,0.5,0.5,0.5], resultSettings=resultSettings[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse MAP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query result to referrence\n",
    "import pprint\n",
    "import operator\n",
    "def compareTo(nlimedResultsFile, dataToCompare, isVerbose=True):\n",
    "    with open(nlimedResultsFile, 'r') as fp:\n",
    "        nliSettings = json.load(fp)\n",
    "    tfModes = {'mode1':'mode_1', 'mode2':'mode_2', 'mode3':'mode_3'}\n",
    "    mapDf = pd.DataFrame(columns=['tfIndex','tfMode', 'parser','mAP@5','mAP@10','mAP'])\n",
    "    commonFound = None\n",
    "    statCommonFound = {k:0 for k,v in dataToCompare.items()}\n",
    "    for idxMode, v1 in nliSettings.items():\n",
    "        if idxMode=='ncbo': continue\n",
    "        for tfMode, v2 in v1.items():\n",
    "            if tfMode == 'ncbo': continue\n",
    "            for parser, v3 in v2.items():\n",
    "                map5 = getMAP(v3['maxAuc100']['results'], dataToCompare,5)\n",
    "                map10 = getMAP(v3['maxAuc100']['results'], dataToCompare,10)\n",
    "                map1000 = getMAP(v3['maxAuc100']['results'], dataToCompare,1000)\n",
    "                mapDf.loc[len(mapDf.index)] = [idxMode, tfModes[tfMode], parser, map5['map'], map10['map'], map1000['map']]\n",
    "    return mapDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate based on the max of terms in query and terms in ontology class proportion\n",
    "mapDf = pd.DataFrame(columns=['tfIndex','tfMode', 'parser','mAP@5','mAP@10','mAP','proportion(p)'])\n",
    "propLabels = {'0':'p=0', '0.5':'0<p<=0.5', '1.0':'0.5<p<=1.0'}\n",
    "propSetting = [0,0.5,1.0]\n",
    "native_QR_PMR_prop ={ps:{} for ps in propSetting}\n",
    "for q, v in native_QR_PMR.items():\n",
    "    prop = native_QR_PMR_max[q]\n",
    "    for ps in propSetting:\n",
    "        if prop <= ps:\n",
    "            native_QR_PMR_prop[ps].update({q:v})\n",
    "            break\n",
    "\n",
    "# Get MAP for each proportion and defferent nlimedResults\n",
    "nlimedResultsFiles=[\n",
    "                   'saveFile/nlimedSummaryResultsComplete_0_5.json',\n",
    "                   'saveFile/nlimedSummaryResultsComplete_3_3_1_1_1_0_5.json',\n",
    "                   'saveFile/nlimedSummaryResultsComplete_3_3_0.5_0.5_0.5_0_5.json',\n",
    "                    ]\n",
    "for nlimedResultsFile in nlimedResultsFiles:\n",
    "    for prop, native_prop in native_QR_PMR_prop.items():\n",
    "        df = compareTo(nlimedResultsFile, native_prop, isVerbose=False)\n",
    "        df['proportion(p)'] = [propLabels[str(prop)]] * len(df.index)\n",
    "        mapDf = mapDf.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "p = sns.catplot(x='proportion(p)',y='mAP@10', \n",
    "            hue='parser', aspect=1, height=4, \n",
    "            row='tfMode', col='tfIndex', \n",
    "            margin_titles=True,\n",
    "            data=mapDf,\n",
    "            hue_order = ['benepar','stanza','coreNLP','xStanza'],\n",
    "            kind = 'box', \n",
    "#             split=True;\n",
    "           )\n",
    "# sns.set_theme(style=\"ticks\")\n",
    "p.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "p._legend.set_title('')\n",
    "plt.savefig('saveFigures/nlimed_native_behaviour.pdf',dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "p = sns.catplot(x='proportion(p)',y='mAP@10', \n",
    "            hue='parser', aspect=1, height=4, \n",
    "            # row='tfMode', col='tfIndex', \n",
    "            margin_titles=True,\n",
    "            data=mapDf,\n",
    "            hue_order = ['benepar','stanza','coreNLP','xStanza'],\n",
    "            kind = 'box', \n",
    "           )\n",
    "# sns.set_theme(style=\"ticks\")\n",
    "p.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "p._legend.set_title('')\n",
    "plt.savefig('saveFigures/nlimed_native_behaviour.pdf',dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
